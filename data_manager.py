import pandas as pd
import yfinance as yf
import json
import os
from datetime import datetime, timedelta
import time

# --- ì„¤ì • ---
USER_CSV_FILE = "etf_info.csv"
DATA_DIR = "data"
PRICE_DATA_DIR = os.path.join(DATA_DIR, "fund_prices")
CSV_FILE_PATH = os.path.join(DATA_DIR, USER_CSV_FILE)
MASTER_FILE_PATH = os.path.join(DATA_DIR, "etf_master.json")
ASSET_PAIRS_PATH = os.path.join(DATA_DIR, 'asset_pairs.json')

def convert_csv_to_master_json():
    """
    CSVë¥¼ ì½ì–´ 'score', 'code' ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ê³  ìµœì¢… etf_master.jsonì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"'{CSV_FILE_PATH}' íŒŒì¼ì„ ì½ì–´ ìµœì¢… ë§ˆìŠ¤í„° íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    if not os.path.exists(CSV_FILE_PATH):
        print(f"ì˜¤ë¥˜: '{CSV_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return False
    try:
        try:
            df = pd.read_csv(CSV_FILE_PATH, encoding='utf-8-sig')
        except UnicodeDecodeError:
            df = pd.read_csv(CSV_FILE_PATH, encoding='cp949')

        df.columns = [col.lower() for col in df.columns]

        # --- ë°ì´í„° ì „ì²˜ë¦¬ ---
        df.rename(columns={'saaclass': 'saa_class', 'taaclass': 'taa_class'}, inplace=True)
        df['ë‹¨ì¶•ì½”ë“œ'] = df['ë‹¨ì¶•ì½”ë“œ'].astype(str).str.zfill(6)
        df['ticker'] = df['ë‹¨ì¶•ì½”ë“œ'].astype(str).str.zfill(6) + '.KS'
        df['ìƒì¥ì¼'] = pd.to_datetime(df['ìƒì¥ì¼'], errors='coerce')
        df = df.dropna(subset=['ìƒì¥ì¼', 'saa_class', 'taa_class'])
        df = df[~df['saa_class'].isin(['ë¯¸ë¶„ë¥˜', ''])]

        # --- 2-1. Score ìƒì„± ---
        df = df.sort_values(by='ìƒì¥ì¼')
        df['score'] = df.groupby(['saa_class', 'taa_class']).cumcount() + 1
        print("âœ… 'score' ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ.")

        # --- 2-2. Code ìƒì„± ---
        saa_prefix_map = {
            'êµ­ë‚´ì£¼ì‹': 'SK', 'êµ­ë‚´ì±„ê¶Œ': 'BK', 'í•´ì™¸ì£¼ì‹': 'SG',
            'í•´ì™¸ì±„ê¶Œ': 'BG', 'ëŒ€ì²´íˆ¬ì': 'AI', 'ë‹¨ê¸°ìê¸ˆ': 'MM'
        }
        
        unique_pairs = df[['saa_class', 'taa_class']].drop_duplicates().sort_values(
            by=['saa_class', 'taa_class']
        ).reset_index(drop=True)
        
        unique_pairs['pair_rank'] = unique_pairs.groupby('saa_class').cumcount() + 1
        unique_pairs['code'] = (
            unique_pairs['saa_class'].map(saa_prefix_map) + 
            unique_pairs['pair_rank'].apply(lambda x: f'{x:02d}')
        )

        df = df.merge(unique_pairs[['saa_class', 'taa_class', 'code']], on=['saa_class', 'taa_class'], how='left')
        print("âœ… 'code' ì»¬ëŸ¼ ìƒì„± ì™„ë£Œ.")
        
        # --- íŒŒì¼ ì €ì¥ ì „, ë‚ ì§œ íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ ---
        # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ì´ í•œ ì¤„ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”! ğŸ‘‡ğŸ‘‡ğŸ‘‡
        df['ìƒì¥ì¼'] = df['ìƒì¥ì¼'].dt.strftime('%Y-%m-%d')

        # --- íŒŒì¼ ì €ì¥ ---
        etf_list = df.to_dict('records')
        with open(MASTER_FILE_PATH, 'w', encoding='utf-8') as f:
            json.dump(etf_list, f, ensure_ascii=False, indent=4)
        print(f"ì„±ê³µ! '{MASTER_FILE_PATH}' íŒŒì¼ì´ ìƒì„±/ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"ì˜¤ë¥˜: CSV íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤ - {e}")
        import traceback
        traceback.print_exc()
        return False

def create_asset_pairs():
    """etf_master.jsonì—ì„œ SAA/TAA ì¡°í•©ì„ ì¶”ì¶œí•˜ì—¬ asset_pairs.jsonì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"'{MASTER_FILE_PATH}' íŒŒì¼ì„ ì½ì–´ ìì‚° ì¡°í•©ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        with open(MASTER_FILE_PATH, 'r', encoding='utf-8') as f:
            etf_list = json.load(f)
        
        df = pd.DataFrame(etf_list)
        asset_pairs_df = df[['saa_class', 'taa_class']].drop_duplicates().dropna()
        result = asset_pairs_df.to_dict('records')
        
        with open(ASSET_PAIRS_PATH, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=4)
        print(f"ì„±ê³µ! ì´ {len(result)}ê°œì˜ ìì‚° ì¡°í•©ì„ '{ASSET_PAIRS_PATH}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: {e}")

def update_etf_prices_as_csv():
    """etf_master.jsonì„ ê¸°ì¤€ìœ¼ë¡œ ê°œë³„ ETF ê°€ê²© ë°ì´í„°ë¥¼ CSVë¡œ ì €ì¥/ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    # (ì´ í•¨ìˆ˜ëŠ” ê¸°ì¡´ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€)
    os.makedirs(PRICE_DATA_DIR, exist_ok=True)
    if not os.path.exists(MASTER_FILE_PATH):
        print(f"ì˜¤ë¥˜: '{MASTER_FILE_PATH}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    with open(MASTER_FILE_PATH, 'r', encoding='utf-8') as f:
        etf_list = json.load(f)

    print(f"\nì´ {len(etf_list)}ê°œ ETFì˜ ê°€ê²© ë°ì´í„° ì—…ë°ì´íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (CSV í˜•ì‹)...")
    
    for i, etf in enumerate(etf_list):
        ticker = etf.get('ticker')
        if not ticker: continue
        
        file_path = os.path.join(PRICE_DATA_DIR, f"{ticker}.csv")
        start_date_str = (datetime.now() - timedelta(days=10*365)).strftime('%Y-%m-%d')
        
        if os.path.exists(file_path):
            try:
                df_existing = pd.read_csv(file_path, index_col='Date', parse_dates=True)
                if not df_existing.empty:
                    start_date_str = (df_existing.index.max() + timedelta(days=1)).strftime('%Y-%m-%d')
            except Exception: pass
        
        if pd.to_datetime(start_date_str).date() >= datetime.now().date():
            print(f"({i+1}/{len(etf_list)}) {ticker}: ì´ë¯¸ ìµœì‹  ë°ì´í„°ì…ë‹ˆë‹¤.")
            continue
            
        try:
            time.sleep(0.2)
            print(f"({i+1}/{len(etf_list)}) {ticker}: {start_date_str}ë¶€í„° ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘...")
            df_new = yf.download(ticker, start=start_date_str, progress=False)
            
            if df_new.empty:
                print(f" -> {ticker}: ë°ì´í„° ì—†ìŒ")
                continue

            header = not os.path.exists(file_path)
            df_new.to_csv(file_path, mode='a' if not header else 'w', header=header, index=True)
            print(f" -> {ticker}: ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f" -> ì˜¤ë¥˜: {ticker} ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨ - {e}")

    print("\nê°œë³„ ETF ê°€ê²© ë°ì´í„° ì—…ë°ì´íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


def create_synthetic_indices():
    """
    'code'ë³„ë¡œ í•©ì„± ì§€ìˆ˜ë¥¼ ìƒì„±í•˜ì—¬ 'code.csv' íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤. (ì‹œì‘ì¼ ìµœì í™” ë²„ì „)
    """
    print("\ní•©ì„± ì§€ìˆ˜(synthetic index) ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    if not os.path.exists(MASTER_FILE_PATH):
        print(f"ì˜¤ë¥˜: '{MASTER_FILE_PATH}'ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return

    master_df = pd.read_json(MASTER_FILE_PATH)
    master_df['ìƒì¥ì¼'] = pd.to_datetime(master_df['ìƒì¥ì¼'])
    
    price_data = {}
    print("ëª¨ë“  ETFì˜ ì¼ì¼ ìˆ˜ìµë¥ ì„ ê³„ì‚° ì¤‘ì…ë‹ˆë‹¤...")
    for ticker in master_df['ticker'].unique():
        file_path = os.path.join(PRICE_DATA_DIR, f"{ticker}.csv")
        if not os.path.exists(file_path):
            continue
        
        try:
            df = pd.read_csv(file_path, skiprows=2)
            df.columns = ['Date', 'Close', 'High', 'Low', 'Open', 'Volume']
            df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
            df = df.dropna(subset=['Date'])
            df.set_index('Date', inplace=True)
            df['Close'] = pd.to_numeric(df['Close'], errors='coerce')
            df = df.dropna(subset=['Close'])

            if len(df) > 1:
                df = df[~df.index.duplicated(keep='first')]
                df.sort_index(inplace=True)
                price_data[ticker] = df['Close'].pct_change()

        except Exception as e:
            print(f"ê²½ê³ : {ticker}.csv íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")

    unique_codes = master_df['code'].dropna().unique()
    print(f"ì´ {len(unique_codes)}ê°œì˜ í•©ì„± ì§€ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")

    for code in unique_codes:
        try:
            print(f" - ì§€ìˆ˜ '{code}' ìƒì„± ì¤‘...")
            group_df = master_df[master_df['code'] == code]
            
            # â˜…â˜…â˜…â˜…â˜… [ìˆ˜ì •ëœ ë¡œì§ ì‹œì‘] â˜…â˜…â˜…â˜…â˜…
            # ê·¸ë£¹ ë‚´ ETFë“¤ì˜ 'ê³µì‹ ìƒì¥ì¼'ì´ ì•„ë‹Œ 'ì‹¤ì œ ë°ì´í„° ì‹œì‘ì¼' ì¤‘ ê°€ì¥ ë¹ ë¥¸ ë‚ ì§œë¥¼ ì°¾ìŠµë‹ˆë‹¤.
            actual_start_date = None
            for ticker in group_df['ticker']:
                if ticker in price_data and not price_data[ticker].empty:
                    # ë°ì´í„°ê°€ ìˆëŠ” ì²« ë‚ ì§œ
                    first_valid_date = price_data[ticker].first_valid_index()
                    
                    # âœ… None ê°’ ì²´í¬ ì¶”ê°€
                    if first_valid_date is not None:
                        if actual_start_date is None or first_valid_date < actual_start_date:
                            actual_start_date = first_valid_date
            
            # ì‹¤ì œ ë°ì´í„°ê°€ ì „í˜€ ì—†ëŠ” ê·¸ë£¹ì´ë¼ë©´ ê±´ë„ˆëœë‹ˆë‹¤.
            if actual_start_date is None:
                print(f"   -> ê²½ê³ : '{code}' ê·¸ë£¹ì— ìœ íš¨í•œ ê°€ê²© ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
                continue
            
            end_date = datetime.now().date()
            # ì°¾ì€ 'ì‹¤ì œ ì‹œì‘ì¼'ë¶€í„° ë‚ ì§œ ë²”ìœ„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
            date_range = pd.date_range(start=actual_start_date, end=end_date, freq='B')
            # â˜…â˜…â˜…â˜…â˜… [ìˆ˜ì •ëœ ë¡œì§ ë] â˜…â˜…â˜…â˜…â˜…
            
            daily_avg_returns = []
            
            for dt in date_range:
                # ê³µì‹ ìƒì¥ì¼ì´ ì•„ë‹Œ, ì‹¤ì œ ë‚ ì§œ(dt)ë¥¼ ê¸°ì¤€ìœ¼ë¡œ active ETFë¥¼ íŒë‹¨í•©ë‹ˆë‹¤.
                active_etfs = group_df[group_df['ìƒì¥ì¼'] <= dt]
                
                returns_for_day = []
                for ticker in active_etfs['ticker']:
                    # í•´ë‹¹ ë‚ ì§œì— ìˆ˜ìµë¥  ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
                    if ticker in price_data and dt in price_data[ticker].index:
                        ret = price_data[ticker].loc[dt]
                        if pd.notna(ret):
                            returns_for_day.append(ret)
                
                if returns_for_day:
                    avg_return = sum(returns_for_day) / len(returns_for_day)
                    daily_avg_returns.append(avg_return)
                else:
                    # ì£¼ë§/íœ´ì¼ ë“± ëª¨ë“  ETF ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° 0ìœ¼ë¡œ ì²˜ë¦¬
                    daily_avg_returns.append(0.0)

            index_df = pd.DataFrame({'Date': date_range, 'return': daily_avg_returns})
            index_df.set_index('Date', inplace=True)
            
            # ì²« ë‚ ì˜ ìˆ˜ìµë¥ ì€ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê¸°ì¤€ì (100)ì„ ë§Œë“­ë‹ˆë‹¤.
            if not index_df.empty:
                index_df.iloc[0, index_df.columns.get_loc('return')] = 0.0

            index_df['close'] = 100 * (1 + index_df['return']).cumprod()
            
            output_path = os.path.join(PRICE_DATA_DIR, f"{code}.csv")
            index_df[['close', 'return']].to_csv(output_path)
            print(f"   -> ì„±ê³µ: '{output_path}'ì— ì €ì¥ ì™„ë£Œ. (ì‹œì‘ì¼: {actual_start_date.date()})")

        except Exception as e:
            print(f"   -> ì˜¤ë¥˜: '{code}' ì§€ìˆ˜ ìƒì„± ì‹¤íŒ¨ - {e}")
            # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ì •ë³´ ì¶œë ¥
            import traceback
            traceback.print_exc()
    
    print("\nëª¨ë“  í•©ì„± ì§€ìˆ˜ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
if __name__ == '__main__':
    if convert_csv_to_master_json():
        create_asset_pairs()
        update_etf_prices_as_csv()
        create_synthetic_indices()