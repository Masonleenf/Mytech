import os
import re
import io
import json
import time
import numpy as np
import pandas as pd
import yfinance as yf
import requests
import urllib3
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple

# SSL ê²½ê³  ë¹„í™œì„±í™”
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


# =============================================================================
# Constants
# =============================================================================

# ETF í‹°ì»¤ ì†ŒìŠ¤
ETF_SOURCES = [
    {
        "name": "DumbStockAPI",
        "url": "https://dumbstockapi.com/stock?exchanges=NYSE,NASDAQ,AMEX&ticker_type=ETF&format=csv"
    },
    {
        "name": "NASDAQ Traded",
        "url": "http://www.nasdaqtrader.com/dynamic/SymDir/nasdaqtraded.txt",
        "sep": "|"
    },
    {
        "name": "GitHub Backup",
        "url": "https://raw.githubusercontent.com/rreichel3/US-Stock-Symbols/main/etf/etf_list.csv"
    }
]

# ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤ ETF íŒ¨í„´
LEVERAGED_PATTERNS = [
    r'\b2x\b', r'\b3x\b', r'\b-1x\b', r'\b-2x\b', r'\b-3x\b',
    r'\bUltra\b', r'\bUltraShort\b', r'\bUltraPro\b',
    r'\bBull\s*2x\b', r'\bBull\s*3x\b', r'\bBear\s*1x\b', r'\bBear\s*2x\b', r'\bBear\s*3x\b',
    r'\bDouble\b', r'\bTriple\b',
    r'\bLeveraged\b', r'\bInverse\b',
]

LEVERAGED_TICKERS = {
    'TQQQ', 'SQQQ', 'UPRO', 'SPXU', 'SPXS', 'SOXL', 'SOXS', 'LABU', 'LABD',
    'NUGT', 'DUST', 'JNUG', 'JDST', 'UVXY', 'SVXY', 'VXX', 'VIXY',
    'TNA', 'TZA', 'FAS', 'FAZ', 'ERX', 'ERY', 'GUSH', 'DRIP',
    'TECL', 'TECS', 'UDOW', 'SDOW', 'UMDD', 'SMDD', 'URTY', 'SRTY',
    'TMF', 'TMV', 'TBT', 'BOIL', 'KOLD', 'UCO', 'SCO', 'UNG', 'DGAZ',
    'YINN', 'YANG', 'EDC', 'EDZ', 'INDL', 'RUSL', 'RUSS',
    'CURE', 'PILL', 'WEBL', 'WEBS', 'NAIL', 'CLAW',
    'FNGU', 'FNGD', 'DPST', 'DRN', 'DRV',
}

# ê¸°ë³¸ ETF ë¦¬ìŠ¤íŠ¸ (ì†ŒìŠ¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ)
DEFAULT_ETFS = [
    'SPY', 'IVV', 'VOO', 'QQQ', 'VTI', 'VEA', 'IEFA', 'VWO', 'AGG', 'BND',
    'IEMG', 'IJH', 'IWF', 'GLD', 'VUG', 'IJR', 'VIG', 'VTV', 'BNDX', 'VXUS',
    'IWM', 'VO', 'IWD', 'XLK', 'VGT', 'VB', 'TLT', 'IVW', 'VNQ', 'LQD',
    'SCHD', 'JEPI', 'JEPQ', 'DGRO', 'VYM', 'XLV', 'XLF', 'XLE', 'XLY', 'XLI'
]


class FinancialDataManager:
    """
    ê¸ˆìœµ ë°ì´í„° ê´€ë¦¬ì.
    
    ETF ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ì§‘, ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ, ë°°ë‹¹ ì •ë³´ ì²˜ë¦¬,
    ê³µë¶„ì‚° ê³„ì‚° ë“± ì „ì²´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, output_dir='data'):
        """
        Initialize the FinancialDataManager.
        
        Args:
            output_dir (str): Directory where output files will be saved.
        """
        self.output_dir = output_dir
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            print(f"Created directory: {output_dir}")
    
    # =========================================================================
    # ETF Universe Collection (from divid.py)
    # =========================================================================
    
    def get_etf_universe(self) -> List[str]:
        """
        ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ETF í‹°ì»¤ ëª©ë¡ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Returns:
            List of ETF ticker symbols
        """
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
        
        for source in ETF_SOURCES:
            print(f"ğŸ“¥ {source['name']} ë‹¤ìš´ë¡œë“œ ì¤‘...")
            try:
                response = requests.get(
                    source['url'], 
                    headers=headers, 
                    timeout=10, 
                    verify=False
                )
                
                if response.status_code != 200:
                    print(f"   âš ï¸ ì‘ë‹µ ì½”ë“œ: {response.status_code}")
                    continue
                
                content = response.content.decode('utf-8')
                sep = source.get('sep', ',')
                df = pd.read_csv(io.StringIO(content), sep=sep)
                
                # ì†ŒìŠ¤ë³„ ì»¬ëŸ¼ ì²˜ë¦¬
                tickers = []
                if 'ticker' in df.columns:
                    tickers = df['ticker'].tolist()
                elif 'Symbol' in df.columns:
                    if 'ETF' in df.columns:
                        df = df[df['ETF'] == 'Y']
                    tickers = df['Symbol'].tolist()
                elif 'symbol' in df.columns:
                    tickers = df['symbol'].tolist()
                
                # ì •ì œ
                clean_tickers = [str(t).strip() for t in tickers if str(t).isalpha()]
                
                if clean_tickers:
                    print(f"   âœ… {len(clean_tickers)}ê°œ í‹°ì»¤ ë°œê²¬")
                    return clean_tickers
                    
            except Exception as e:
                print(f"   âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        print("âš ï¸ ëª¨ë“  ì†ŒìŠ¤ ì‹¤íŒ¨. ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©.")
        return DEFAULT_ETFS.copy()
    
    def filter_by_market_cap(self, tickers: List[str], threshold: int = 300_000_000) -> List[str]:
        """
        ì‹œê°€ì´ì•¡ ê¸°ì¤€ìœ¼ë¡œ ETFë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
        
        Args:
            tickers: List of ticker symbols
            threshold: Minimum market cap in USD (default: 300M)
        
        Returns:
            List of tickers meeting the threshold
        """
        print(f"\nğŸ” ì‹œê°€ì´ì•¡ {threshold:,} USD ì´ìƒ í•„í„°ë§ ({len(tickers)}ê°œ)...")
        
        qualified = []
        chunk_size = 50
        
        for i in range(0, len(tickers), chunk_size):
            chunk = tickers[i:i+chunk_size]
            print(f"   [{i+len(chunk)}/{len(tickers)}] ì²˜ë¦¬ ì¤‘... (ì„ ì •: {len(qualified)})", end='\r')
            
            try:
                tickers_obj = yf.Tickers(" ".join(chunk))
                
                for symbol in chunk:
                    try:
                        ticker = tickers_obj.tickers[symbol]
                        mc = self._get_market_cap(ticker)
                        
                        if mc and mc >= threshold:
                            qualified.append(symbol)
                    except:
                        continue
            except:
                pass
        
        print(f"\nâœ… í•„í„°ë§ ì™„ë£Œ: {len(qualified)}ê°œ ETF ì„ ì •")
        return qualified
    
    def _get_market_cap(self, ticker_obj) -> Optional[float]:
        """yfinance ê°ì²´ì—ì„œ ì‹œê°€ì´ì•¡ ì¶”ì¶œ"""
        # fast_info ì‹œë„
        try:
            mc = ticker_obj.fast_info.market_cap
            if mc and mc > 0:
                return mc
        except:
            pass
        
        try:
            mc = ticker_obj.fast_info['market_cap']
            if mc and mc > 0:
                return mc
        except:
            pass
        
        # info ì‹œë„
        try:
            mc = ticker_obj.info.get('marketCap')
            if mc and mc > 0:
                return mc
        except:
            pass
        
        return None
    
    # =========================================================================
    # Leveraged/Inverse ETF Filter (from recalculate_yields.py)
    # =========================================================================
    
    def is_leveraged_etf(self, ticker: str, name: str = "") -> bool:
        """
        ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤ ETF ì—¬ë¶€ë¥¼ íŒë³„í•©ë‹ˆë‹¤.
        
        Args:
            ticker: ETF ticker symbol
            name: ETF name (optional)
        
        Returns:
            True if leveraged/inverse ETF
        """
        if ticker in LEVERAGED_TICKERS:
            return True
        
        if name:
            for pattern in LEVERAGED_PATTERNS:
                if re.search(pattern, name, re.IGNORECASE):
                    return True
            
            # Direxion DailyëŠ” í•­ìƒ ë ˆë²„ë¦¬ì§€
            if 'Direxion' in name and ('Daily' in name or 'Ultra' in name or 'Short' in name):
                return True
        
        return False
    
    def filter_leveraged_etfs(self, summary: List[Dict]) -> Tuple[List[Dict], List[str]]:
        """
        ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤ ETFë¥¼ ì œê±°í•©ë‹ˆë‹¤.
        
        Args:
            summary: List of ETF summary dictionaries
        
        Returns:
            Tuple of (filtered_summary, removed_tickers)
        """
        filtered = []
        removed = []
        
        for item in summary:
            ticker = item.get('Ticker', item.get('ticker', ''))
            name = item.get('Name', '')
            
            if self.is_leveraged_etf(ticker, name):
                removed.append(ticker)
            else:
                filtered.append(item)
        
        if removed:
            print(f"ğŸš« ë ˆë²„ë¦¬ì§€/ì¸ë²„ìŠ¤ ETF {len(removed)}ê°œ ì œê±°: {', '.join(removed[:10])}...")
        
        return filtered, removed
    
    # =========================================================================
    # Dividend Yield Calculation (from recalculate_yields.py)
    # =========================================================================
    
    def infer_dividend_frequency(self, schedule: List[Dict]) -> int:
        """
        ë°°ë‹¹ ìŠ¤ì¼€ì¤„ì—ì„œ ë¹ˆë„ë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤.
        
        Args:
            schedule: List of dividend payments with 'date' keys
        
        Returns:
            Annualization factor (12=monthly, 4=quarterly, 2=semi, 1=annual, 0=none)
        """
        if not schedule:
            return 0
        
        # ìµœê·¼ 12ê°œì›” ë°°ë‹¹ í•„í„°ë§
        now = datetime.now()
        one_year_ago = now - timedelta(days=365)
        
        recent = []
        for div in schedule:
            try:
                div_date = datetime.strptime(div['date'], '%Y-%m-%d')
                if div_date >= one_year_ago:
                    recent.append(div_date)
            except:
                pass
        
        count = len(recent)
        
        if count >= 11:
            return 12  # Monthly
        elif count >= 4:
            return 4   # Quarterly
        elif count >= 2:
            return 2   # Semi-annual
        elif count >= 1:
            return 1   # Annual
        
        # ë‚ ì§œ ê°„ê²©ìœ¼ë¡œ ì¶”ë¡ 
        if len(schedule) >= 2:
            try:
                dates = sorted(
                    [datetime.strptime(d['date'], '%Y-%m-%d') for d in schedule],
                    reverse=True
                )
                gaps = [(dates[i] - dates[i+1]).days for i in range(min(4, len(dates)-1))]
                avg_gap = sum(gaps) / len(gaps) if gaps else 365
                
                if avg_gap < 45:
                    return 12
                elif avg_gap < 120:
                    return 4
                elif avg_gap < 250:
                    return 2
            except:
                pass
        
        return 1
    
    def calculate_dividend_yield(
        self, 
        schedule: List[Dict], 
        price_series: pd.Series
    ) -> Tuple[float, str]:
        """
        ì •í™•í•œ ë°°ë‹¹ ìˆ˜ìµë¥ ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        
        Formula: Yield = (Dividend / Price_at_ExDiv) Ã— Frequency Ã— 100
        
        Args:
            schedule: List of dividend payments [{'date': ..., 'amount': ...}]
            price_series: Price series with DatetimeIndex
        
        Returns:
            Tuple of (yield_percent, detail_message)
        """
        if not schedule:
            return 0.0, "No dividends"
        
        if price_series is None or len(price_series) < 10:
            return 0.0, "Insufficient price data"
        
        # ìµœê·¼ ë°°ë‹¹ (ìŠ¤ì¼€ì¤„ì€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ê°€ì •)
        recent_div = schedule[0]
        div_date = recent_div['date']
        div_amount = recent_div.get('amount', 0)
        
        if div_amount <= 0:
            return 0.0, "Invalid dividend amount"
        
        # ë°°ë‹¹ë½ì¼ 2ì˜ì—…ì¼ ì „ ê°€ê²©
        price_at_exdiv = self._get_price_before_date(price_series, div_date, 2)
        
        if price_at_exdiv is None or price_at_exdiv <= 0:
            price_at_exdiv = price_series.iloc[-1]
        
        # ë¹ˆë„ ì¶”ë¡ 
        freq = self.infer_dividend_frequency(schedule)
        
        if freq == 0:
            return 0.0, "Unknown frequency"
        
        # ì—°í™˜ì‚° ìˆ˜ìµë¥  ê³„ì‚°
        single_yield = div_amount / price_at_exdiv
        annual_yield = single_yield * freq * 100
        
        # 50% ìƒí•œ (ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ìˆ˜ìµë¥  ì œí•œ)
        if annual_yield > 50:
            # ìµœê·¼ 4ê°œ ë°°ë‹¹ í‰ê·  ì‚¬ìš©
            recent_divs = [d['amount'] for d in schedule[:min(4, len(schedule))]]
            avg_div = sum(recent_divs) / len(recent_divs)
            annual_yield = (avg_div / price_at_exdiv) * freq * 100
            
            if annual_yield > 50:
                detail = f"Capped from {annual_yield:.1f}%"
                annual_yield = 50.0
            else:
                detail = f"Used avg of {len(recent_divs)} divs"
        else:
            detail = f"Freq={freq}, Div=${div_amount:.4f}, Price=${price_at_exdiv:.2f}"
        
        return round(annual_yield, 2), detail
    
    def _get_price_before_date(
        self, 
        price_series: pd.Series, 
        target_date: str, 
        business_days: int = 2
    ) -> Optional[float]:
        """N ì˜ì—…ì¼ ì „ ê°€ê²© ì¡°íšŒ"""
        try:
            target = pd.to_datetime(target_date)
            valid_dates = price_series.index[price_series.index < target]
            
            if len(valid_dates) < business_days:
                return None
            
            return price_series.loc[valid_dates[-business_days]]
        except:
            return None
    
    # =========================================================================
    # Dividend Schedule Collection (from patch_metadata.py)
    # =========================================================================
    
    def fetch_dividend_schedule(self, ticker: str, limit: int = 12) -> List[Dict]:
        """
        yfinanceì—ì„œ ë°°ë‹¹ ìŠ¤ì¼€ì¤„ì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            ticker: ETF ticker symbol
            limit: Maximum number of recent dividends to return
        
        Returns:
            List of dividend payments [{'date': ..., 'amount': ...}]
        """
        try:
            tik = yf.Ticker(ticker)
            divs = tik.dividends
            
            if divs.empty:
                return []
            
            # ìµœê·¼ Nê°œ ë°°ë‹¹, ë‚´ë¦¼ì°¨ìˆœ
            recent = divs.sort_index(ascending=False).head(limit)
            
            schedule = []
            for d_date, d_amt in recent.items():
                schedule.append({
                    "date": d_date.strftime('%Y-%m-%d'),
                    "amount": round(float(d_amt), 4)
                })
            
            return schedule
            
        except Exception as e:
            return []
    
    def get_dividend_data(self, tickers: List[str]) -> List[Dict]:
        """
        ì—¬ëŸ¬ ETFì˜ ë°°ë‹¹ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
        
        Args:
            tickers: List of ticker symbols
        
        Returns:
            List of dividend data dictionaries
        """
        print(f"\nğŸ’° {len(tickers)}ê°œ ETF ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
        results = []
        
        for i, symbol in enumerate(tickers):
            print(f"   [{i+1}/{len(tickers)}] {symbol}...", end='\r')
            
            try:
                ticker = yf.Ticker(symbol)
                
                # Yield
                yield_val = 0
                try:
                    yield_val = ticker.info.get('dividendYield', 0)
                    if yield_val is None:
                        yield_val = 0
                except:
                    pass
                
                # Schedule
                schedule = self.fetch_dividend_schedule(symbol)
                
                results.append({
                    'ticker': symbol,
                    'dividend_yield': yield_val,
                    'dividend_schedule': schedule,
                    'last_updated': datetime.now().strftime('%Y-%m-%d')
                })
                
            except:
                pass
        
        print(f"\nâœ… ë°°ë‹¹ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ: {len(results)}ê°œ")
        return results
    
    # =========================================================================
    # Data Loading
    # =========================================================================
    
    def load_tickers_from_file(self, file_path: str) -> List[Dict]:
        """Load the base metadata from JSON file."""
        if not os.path.exists(file_path):
            print(f"Error: File {file_path} not found.")
            return []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            print(f"Loaded {len(data)} items from {file_path}")
            return data
    
    # =========================================================================
    # Market Data Download
    # =========================================================================
    
    def download_data_batch(self, tickers: List[str], chunk_size=100, period="5y") -> pd.DataFrame:
        """
        Download data in batches to handle large lists of tickers.
        """
        all_dfs = []
        total_tickers = len(tickers)
        
        print(f"ğŸ“Š {total_tickers}ê°œ í‹°ì»¤ ë‹¤ìš´ë¡œë“œ ì‹œì‘ (ì²­í¬ í¬ê¸°: {chunk_size})...")
        
        for i in range(0, total_tickers, chunk_size):
            chunk = tickers[i : i + chunk_size]
            print(f"   ì²­í¬ {i//chunk_size + 1}/{(total_tickers + chunk_size - 1) // chunk_size} ({len(chunk)}ê°œ)...")
            
            try:
                df = yf.download(chunk, period=period, group_by='ticker', threads=True, progress=False)
                
                if not df.empty:
                    all_dfs.append(df)
                
                time.sleep(1.0)
                
            except Exception as e:
                print(f"   âš ï¸ ì²­í¬ {i} ì˜¤ë¥˜: {e}")
        
        if not all_dfs:
            return pd.DataFrame()
        
        print("   ë°ì´í„° ë³‘í•© ì¤‘...")
        full_df = pd.concat(all_dfs, axis=1)
        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ. Shape: {full_df.shape}")
        
        return full_df
    
    # =========================================================================
    # Market Data Processing
    # =========================================================================
    
    def process_market_data(self, price_df: pd.DataFrame, file_name='market_data.parquet') -> pd.DataFrame:
        """
        Save formatted market data to Parquet.
        Transforms (Date, Ticker-Levels) -> (Date, Ticker-Attributes) wide format.
        """
        print("ğŸ“ ë§ˆì¼“ ë°ì´í„° ì²˜ë¦¬ ì¤‘...")
        
        try:
            if not isinstance(price_df.columns, pd.MultiIndex):
                print("Warning: DataFrame is not MultiIndex.")
                return price_df
            
            def get_level_df(df, col_name):
                if col_name in df.columns.get_level_values(1):
                    return df.xs(col_name, axis=1, level=1, drop_level=True)
                return pd.DataFrame(index=df.index)
            
            adj_close = get_level_df(price_df, 'Adj Close')
            close = get_level_df(price_df, 'Close')
            
            # Adj Close ìš°ì„ , ì—†ìœ¼ë©´ Close ì‚¬ìš©
            best_price = adj_close.combine_first(close)
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            returns_df = best_price.pct_change()
            
            # MultiIndex ì¬êµ¬ì„±
            if not returns_df.empty:
                tuples = [(ticker, 'Daily_Return') for ticker in returns_df.columns]
                returns_cols = pd.MultiIndex.from_tuples(tuples, names=price_df.columns.names)
                returns_df.columns = returns_cols
            
            # ì›ë³¸ ë°ì´í„° í•„í„°ë§ (Close, Adj Closeë§Œ)
            cols_to_keep = []
            if 'Close' in price_df.columns.get_level_values(1):
                cols_to_keep.append('Close')
            if 'Adj Close' in price_df.columns.get_level_values(1):
                cols_to_keep.append('Adj Close')
            
            idx = pd.IndexSlice
            base_data = price_df.loc[:, idx[:, cols_to_keep]]
            
            # ë³‘í•©
            final_df = pd.concat([base_data, returns_df], axis=1)
            final_df = final_df.sort_index(axis=1, level=0)
            
            # ì €ì¥
            parquet_path = os.path.join(self.output_dir, file_name)
            final_df.to_parquet(parquet_path, engine='pyarrow', compression='snappy')
            
            print(f"âœ… ì €ì¥ ì™„ë£Œ: {parquet_path} (Shape: {final_df.shape})")
            return final_df
            
        except Exception as e:
            print(f"âš ï¸ ë§ˆì¼“ ë°ì´í„° ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            return price_df
    
    # =========================================================================
    # Covariance Matrix (enhanced from recalculate_covariance.py)
    # =========================================================================
    
    def process_covariance(self, market_data_df: pd.DataFrame, window_days=252) -> List[str]:
        """
        Calculate Covariance Matrix using Adj Close for Total Return.
        Returns list of valid tickers that were included.
        """
        print("ğŸ“Š ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚° ì¤‘ (Adj Close ê¸°ë°˜)...")
        
        try:
            # Adj Close ì¶”ì¶œ (Total Return ë°˜ì˜)
            if isinstance(market_data_df.columns, pd.MultiIndex):
                if 'Adj Close' in market_data_df.columns.get_level_values(1):
                    price_df = market_data_df.xs('Adj Close', axis=1, level=1)
                elif 'Close' in market_data_df.columns.get_level_values(1):
                    print("   âš ï¸ Adj Close ì—†ìŒ, Close ì‚¬ìš©")
                    price_df = market_data_df.xs('Close', axis=1, level=1)
                else:
                    print("   âŒ ê°€ê²© ë°ì´í„° ì—†ìŒ")
                    return []
            else:
                price_df = market_data_df
            
            # ìˆ˜ìµë¥  ê³„ì‚°
            returns = price_df.pct_change()
            
            # ìµœê·¼ Nì¼ í•„í„°
            recent_returns = returns.tail(window_days)
            
            # NaN ì—†ëŠ” í‹°ì»¤ë§Œ ì‚¬ìš©
            valid_returns = recent_returns.dropna(axis=1, how='any')
            
            if valid_returns.empty:
                print("   âŒ ìœ íš¨í•œ í‹°ì»¤ ì—†ìŒ")
                return []
            
            # ê³µë¶„ì‚° ê³„ì‚°
            cov_matrix = valid_returns.cov()
            
            # ì €ì¥
            npy_path = os.path.join(self.output_dir, 'covariance.npy')
            np.save(npy_path, cov_matrix.values)
            
            valid_tickers = valid_returns.columns.tolist()
            print(f"âœ… ê³µë¶„ì‚° ì €ì¥: {npy_path} (Shape: {cov_matrix.shape}, {len(valid_tickers)}ê°œ í‹°ì»¤)")
            
            return valid_tickers
            
        except Exception as e:
            print(f"âš ï¸ ê³µë¶„ì‚° ê³„ì‚° ì˜¤ë¥˜: {e}")
            return []
    
    # =========================================================================
    # Metrics Calculation
    # =========================================================================
    
    def calculate_metrics_for_ticker(self, ticker_series: pd.Series) -> Dict[str, float]:
        """
        Calculate singular metrics for a price series.
        """
        try:
            series = ticker_series.dropna()
            if len(series) < 30:
                return {}
            
            start_price = series.iloc[0]
            end_price = series.iloc[-1]
            
            # CAGR
            days = (series.index[-1] - series.index[0]).days
            years = days / 365.25
            cagr = 0.0
            if years > 0 and start_price > 0:
                cagr = (end_price / start_price) ** (1 / years) - 1
            
            # Volatility
            daily_ret = series.pct_change().dropna()
            volatility = daily_ret.std() * np.sqrt(252)
            
            # Max Drawdown
            peak = series.cummax()
            drawdown = (series - peak) / peak
            max_drawdown = drawdown.min()
            
            return {
                "cagr_price_5y": round(cagr, 4),
                "volatility": round(volatility, 4),
                "max_drawdown": round(max_drawdown, 4)
            }
        except:
            return {}
    
    # =========================================================================
    # Summary Generation
    # =========================================================================
    
    def process_full_summary(
        self, 
        valid_tickers_for_cov: List[str], 
        price_df: pd.DataFrame, 
        original_data: List[Dict],
        file_name='etf_summary.json'
    ):
        """
        Generate final summary JSON with accurate dividend yields.
        """
        print("ğŸ“‹ Summary ìƒì„± ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„° ë§µ
        orig_map = {item.get('ticker', item.get('Ticker', '')): item for item in original_data}
        
        summary_list = []
        
        for idx, ticker in enumerate(valid_tickers_for_cov):
            if idx % 50 == 0:
                print(f"   [{idx}/{len(valid_tickers_for_cov)}] ì²˜ë¦¬ ì¤‘...")
            
            base_obj = orig_map.get(ticker)
            if not base_obj:
                continue
            
            try:
                # ê°€ê²© ì‹œë¦¬ì¦ˆ ì¶”ì¶œ
                if isinstance(price_df.columns, pd.MultiIndex):
                    if (ticker, 'Adj Close') in price_df.columns:
                        series = price_df[(ticker, 'Adj Close')]
                    elif (ticker, 'Close') in price_df.columns:
                        series = price_df[(ticker, 'Close')]
                    else:
                        series = pd.Series()
                else:
                    series = price_df.get(ticker, pd.Series())
                
                # ë©”íŠ¸ë¦­ ê³„ì‚°
                metrics = self.calculate_metrics_for_ticker(series)
                
                # ë°°ë‹¹ ìŠ¤ì¼€ì¤„
                schedule = base_obj.get('dividend_schedule', base_obj.get('Dividend Schedule Summary', []))
                
                # ì •í™•í•œ ë°°ë‹¹ ìˆ˜ìµë¥  ê³„ì‚°
                div_yield, _ = self.calculate_dividend_yield(schedule, series)
                
                # ê¸°ì¡´ ìˆ˜ìµë¥ ì´ ìˆê³  ìœ íš¨í•˜ë©´ ì‚¬ìš©, ì•„ë‹ˆë©´ ê³„ì‚°ëœ ê°’ ì‚¬ìš©
                base_yield = base_obj.get('dividend_yield', 0)
                if base_yield and 0 < base_yield < 100:
                    final_yield = base_yield
                else:
                    final_yield = div_yield
                
                new_entry = {
                    "Ticker": ticker,
                    "Name": base_obj.get("Name", base_obj.get("ticker", ticker)),
                    "Key Metrics": {
                        "current_dividend_yield": final_yield,
                        **metrics
                    },
                    "Dividend Schedule Summary": schedule
                }
                
                summary_list.append(new_entry)
                
            except Exception as e:
                print(f"   âš ï¸ {ticker} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        
        # ë ˆë²„ë¦¬ì§€ ETF í•„í„°ë§
        summary_list, removed = self.filter_leveraged_etfs(summary_list)
        
        # ì €ì¥
        json_path = os.path.join(self.output_dir, file_name)
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(summary_list, f, indent=4, ensure_ascii=False)
        
        print(f"âœ… Summary ì €ì¥: {json_path} ({len(summary_list)}ê°œ)")
    
    # =========================================================================
    # Full Pipeline
    # =========================================================================
    
    def run_full_pipeline(
        self,
        market_cap_threshold: int = 300_000_000,
        period: str = "5y",
        use_cache: bool = False
    ) -> None:
        """
        ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            market_cap_threshold: Minimum market cap for ETF filtering
            period: Historical data period (e.g., "5y", "3y")
            use_cache: If True, skip download and use existing data
        """
        print("=" * 60)
        print("ğŸš€ Financial Data Pipeline ì‹œì‘")
        print("=" * 60)
        
        # 1. ETF ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ì§‘
        print("\n[1/5] ETF ìœ ë‹ˆë²„ìŠ¤ ìˆ˜ì§‘...")
        etfs = self.get_etf_universe()
        
        # 2. ì‹œê°€ì´ì•¡ í•„í„°ë§
        print("\n[2/5] ì‹œê°€ì´ì•¡ í•„í„°ë§...")
        filtered_etfs = self.filter_by_market_cap(etfs, market_cap_threshold)
        
        if not filtered_etfs:
            print("âŒ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ETFê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # 3. ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘
        print("\n[3/5] ë°°ë‹¹ ë°ì´í„° ìˆ˜ì§‘...")
        dividend_data = self.get_dividend_data(filtered_etfs)
        
        # 4. ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        print("\n[4/5] ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ...")
        tickers = [d['ticker'] for d in dividend_data]
        price_df = self.download_data_batch(tickers, period=period)
        
        if price_df.empty:
            print("âŒ ê°€ê²© ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
            return
        
        # 5. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥
        print("\n[5/5] ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥...")
        market_data = self.process_market_data(price_df)
        valid_tickers = self.process_covariance(market_data)
        self.process_full_summary(valid_tickers, price_df, dividend_data)
        
        print("\n" + "=" * 60)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("=" * 60)


if __name__ == "__main__":
    import sys
    from db import db_manager
    
    # CLI ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1 and sys.argv[1] == '--update-prices':
        # MongoDB ì—…ë°ì´íŠ¸ ëª¨ë“œ
        print("=" * 60)
        print("ğŸ“Š í•´ì™¸ ë°°ë‹¹ ETF ë°ì´í„° MongoDB ì—…ë°ì´íŠ¸")
        print("=" * 60)
        
        # 1. etf_summary.json â†’ dividend_etf_summary
        summary_path = 'dividend_data/etf_summary.json'
        if os.path.exists(summary_path):
            print(f"\n[1/2] {summary_path} â†’ dividend_etf_summary...")
            with open(summary_path, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)
            
            # ì»¬ë ‰ì…˜ ë¹„ìš°ê³  ìƒˆë¡œ ì‚½ì…
            db_manager.dividend_etf_summary.delete_many({})
            if summary_data:
                db_manager.dividend_etf_summary.insert_many(summary_data)
            print(f"   âœ… {len(summary_data)}ê°œ ETF ì •ë³´ ì—…ë¡œë“œ ì™„ë£Œ")
        else:
            print(f"   âš ï¸ {summary_path} íŒŒì¼ ì—†ìŒ")
        
        # 2. market_data.parquet â†’ dividend_etf_prices
        parquet_path = 'dividend_data/market_data.parquet'
        if os.path.exists(parquet_path):
            print(f"\n[2/2] {parquet_path} â†’ dividend_etf_prices...")
            df = pd.read_parquet(parquet_path)
            
            # MultiIndex ì²˜ë¦¬
            if isinstance(df.columns, pd.MultiIndex):
                # í‹°ì»¤ë³„ ê°€ê²© ë°ì´í„° ì¶”ì¶œ
                tickers = df.columns.get_level_values(0).unique()
                
                db_manager.dividend_etf_prices.delete_many({})
                uploaded = 0
                
                for ticker in tickers:
                    try:
                        ticker_data = df[ticker].copy()
                        if 'Adj Close' in ticker_data.columns:
                            prices = ticker_data[['Adj Close', 'Close']].dropna(how='all')
                        elif 'Close' in ticker_data.columns:
                            prices = ticker_data[['Close']].dropna(how='all')
                        else:
                            continue
                        
                        if prices.empty:
                            continue
                        
                        prices = prices.reset_index()
                        prices['Date'] = prices['Date'].dt.strftime('%Y-%m-%d')
                        prices_list = prices.to_dict('records')
                        
                        doc = {
                            'ticker': ticker,
                            'prices': prices_list,
                            'updated_at': datetime.now()
                        }
                        db_manager.dividend_etf_prices.insert_one(doc)
                        uploaded += 1
                        
                    except Exception as e:
                        print(f"   âš ï¸ {ticker} ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                
                print(f"   âœ… {uploaded}ê°œ ETF ê°€ê²© ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")
            else:
                print("   âš ï¸ MultiIndex í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤")
        else:
            print(f"   âš ï¸ {parquet_path} íŒŒì¼ ì—†ìŒ")
        
        print("\n" + "=" * 60)
        print("âœ… MongoDB ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        print("=" * 60)
    else:
        # ê¸°ë³¸ ëª¨ë“œ: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        manager = FinancialDataManager(output_dir='dividend_data')
        manager.run_full_pipeline()
